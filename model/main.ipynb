{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Relevant Imports ====================\n",
    "\n",
    "# Torch and Torch-related modules for deep learning\n",
    "import torch  # Core PyTorch functionalities\n",
    "from torch import nn\n",
    "torch.autograd.set_detect_anomaly(True)  # Enables anomaly detection for debugging\n",
    "from transformers import ViTHybridModel, AutoModelForImageClassification\n",
    "from torchvision import models  # Pretrained models\n",
    "\n",
    "# General-purpose libraries\n",
    "import os  # OS-level operations (file paths, directory handling)\n",
    "import random  # Random number generation\n",
    "\n",
    "# c libraries\n",
    "from skimage import exposure  # Module for image intensity adjustment and histogram-based operations, such as contrast enhancement and histogram equalization.\n",
    "import cv2  # OpenCV for image processing\n",
    "from PIL import Image  # Image handling\n",
    "import pydicom  # Handling DICOM medical imaging files\n",
    "import pandas as pd  # Data manipulation and analysis\n",
    "import numpy as np  # Numerical computations and array operations\n",
    "import matplotlib.pyplot as plt  # Plotting and visualization\n",
    "\n",
    "# Progress bar visualization\n",
    "from tqdm import tqdm  # Displays progress bars for loops and tasks\n",
    "\n",
    "# Machine learning utilities\n",
    "from sklearn.model_selection import KFold  # Cross-validation splitting strategy\n",
    "\n",
    "# Custom configurations and modules\n",
    "from config import HyperP  # Hyperparameter configuration object\n",
    "\n",
    "# ==========================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the HyperP class with model-specific configurations\n",
    "# 'slope_train_vit_simple' indicates the model is for slope prediction\n",
    "hyp = HyperP(model_type=\"slope_train_vit_simple\")  \n",
    "\n",
    "# Load a range of images from a CSV file into a Pandas DataFrame\n",
    "# Assumes \"images.csv\" contains metadata or a list of images\n",
    "images_range = pd.read_csv(\"images.csv\")\n",
    "\n",
    "# ==================== Set Random Seed for Reproducibility ====================\n",
    "\n",
    "# Retrieve the seed value from the HyperP object\n",
    "seed = hyp.seed  \n",
    "\n",
    "# Set the seed for Python's built-in random module\n",
    "random.seed(seed)  \n",
    "\n",
    "# Set the PYTHONHASHSEED environment variable for hash-based operations \n",
    "# to ensure deterministic results\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)  \n",
    "\n",
    "# Set the seed for NumPy's random number generator\n",
    "np.random.seed(seed)  \n",
    "\n",
    "# Set the seed for PyTorch to ensure reproducibility of results\n",
    "torch.manual_seed(seed)  \n",
    "\n",
    "# Configure PyTorch's CUDA backend for deterministic results\n",
    "# `deterministic = True`: Ensures consistent results by using deterministic algorithms\n",
    "torch.backends.cudnn.deterministic = True  \n",
    "\n",
    "# Disable the use of non-deterministic algorithms to avoid performance trade-offs\n",
    "# `benchmark = False`: Avoids auto-tuning for optimization, ensuring repeatability\n",
    "torch.backends.cudnn.benchmark = False  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the root directory for data from the HyperP configuration object\n",
    "root_path = hyp.data_folder  \n",
    "\n",
    "# Load training metadata from a CSV file located in the data folder\n",
    "# This CSV likely contains information about the training dataset\n",
    "train = pd.read_csv(f'{root_path}/train.csv')  \n",
    "\n",
    "# Filter out rows from the training data where the Patient ID matches specific values\n",
    "# These Patient IDs are likely identified as problematic\n",
    "train_without_badid = train[(train.Patient != 'ID00011637202177653955184') & \n",
    "                             (train.Patient != 'ID00052637202186188008618')]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate tabular features from a DataFrame for a single patient\n",
    "def get_tab(df):\n",
    "    # Initialize a feature vector with the normalized age of the patient\n",
    "    # Normalization is done using the mean and standard deviation of the training data's Age\n",
    "    vector = [(df.Age.values[0] - train.Age.values.mean()) / train.Age.values.std()]\n",
    "    \n",
    "    # Add a binary encoding for the patient's gender\n",
    "    # 0 for 'Male', 1 for 'Female'\n",
    "    if df.Sex.values[0] == 'Male':\n",
    "        vector.append(0)\n",
    "    else:\n",
    "        vector.append(1)\n",
    "    \n",
    "    # Add a one-hot encoded representation for the patient's smoking status\n",
    "    # 'Never smoked' -> [0, 0]\n",
    "    # 'Ex-smoker' -> [1, 1]\n",
    "    # 'Currently smokes' -> [0, 1]\n",
    "    # Any other status -> [1, 0]\n",
    "    if df.SmokingStatus.values[0] == 'Never smoked':\n",
    "        vector.extend([0, 0])\n",
    "    elif df.SmokingStatus.values[0] == 'Ex-smoker':\n",
    "        vector.extend([1, 1])\n",
    "    elif df.SmokingStatus.values[0] == 'Currently smokes':\n",
    "        vector.extend([0, 1])\n",
    "    else:\n",
    "        vector.extend([1, 0])\n",
    "    \n",
    "    # Convert the feature vector to a NumPy array and return it\n",
    "    return np.array(vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and preprocess a medical image from a DICOM file\n",
    "def get_img(path):  \n",
    "    # Read the DICOM file using pydicom\n",
    "    d = pydicom.dcmread(path)\n",
    "    \n",
    "    # Extract the pixel array from the DICOM and enhance contrast using histogram equalization\n",
    "    # Resize the processed image to a fixed resolution of 384x384 using OpenCV\n",
    "    output = cv2.resize(exposure.equalize_hist(d.pixel_array), (384, 384))\n",
    "    \n",
    "    # Return the preprocessed image\n",
    "    return output\n",
    "\n",
    "# Function to load and preprocess a mask image\n",
    "def get_mask(path):\n",
    "    # Open the mask image using PIL (assumes it is a standard image format like PNG or JPEG)\n",
    "    mask = Image.open(path)\n",
    "    \n",
    "    # Convert the mask to a NumPy array and resize it to 384x384\n",
    "    mask = cv2.resize(np.array(mask), (384, 384))\n",
    "    \n",
    "    # Reshape the mask to ensure it has dimensions 384x384 (optional step for consistency)\n",
    "    return mask.reshape(384, 384)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 176/176 [00:00<00:00, 1318.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize dictionaries and list to store results\n",
    "A = {}  # Dictionary to store the slopes (linear regression results) for each patient\n",
    "TAB = {}  # Dictionary to store tabular features for each patient\n",
    "P = []  # List to store patient IDs\n",
    "\n",
    "# Iterate through each unique patient in the training dataset\n",
    "for i, p in enumerate(tqdm(train.Patient.unique())):\n",
    "    # Filter data for the current patient\n",
    "    sub = train.loc[train.Patient == p, :] \n",
    "    \n",
    "    # Extract Forced Vital Capacity (FVC) and Weeks (time points) for the patient\n",
    "    fvc = sub.FVC.values\n",
    "    weeks = sub.Weeks.values\n",
    "    \n",
    "    # For each patient, the code calculates the slope of FVC over time (Weeks) using linear regression, generates tabular features, and stores the results (slope, features, and patient ID) in dictionaries and a list.\n",
    "    c = np.vstack([weeks, np.ones(len(weeks))]).T  # Add a column of ones for the intercept term\n",
    "\n",
    "    a, _ = np.linalg.lstsq(c, fvc, rcond=None)[0]  # Returns the slope 'a' of the best fit line\n",
    "    # ref: https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html\n",
    "\n",
    "    A[p] = a\n",
    "    TAB[p] = get_tab(sub)\n",
    "    P.append(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class for training in PyTorch\n",
    "class OSICData_train(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "        Custom dataset class for loading and processing training data in the OSIC dataset.\n",
    "        \n",
    "        This dataset class filters out bad patient IDs, loads image data, corresponding masks, slopes, and tabular features, \n",
    "        and provides a random sampling of training data.\n",
    "\n",
    "        Attributes:\n",
    "        -----------\n",
    "        BAD_ID : list\n",
    "            A list of patient IDs to exclude from the dataset.\n",
    "        keys : list\n",
    "            A list of patient IDs to include in the dataset after filtering out bad IDs.\n",
    "        a : dict\n",
    "            A dictionary containing slopes for each patient.\n",
    "        tab : dict\n",
    "            A dictionary containing tabular features for each patient.\n",
    "        all_data : list\n",
    "            A list of image file paths for all selected patients.\n",
    "        train_data : dict\n",
    "            A dictionary where the key is a patient ID, and the value is a list of corresponding image file paths.\n",
    "        mask_data : dict\n",
    "            A dictionary where the key is a patient ID, and the value is a list of corresponding mask file paths.\n",
    "        ten_percent : list\n",
    "            A random sample (50%) of the data, used for training.\n",
    "\n",
    "        Methods:\n",
    "        --------\n",
    "        __len__():\n",
    "            Returns the number of samples (50% of available data) in the dataset.\n",
    "        \n",
    "        __getitem__(idx):\n",
    "            Retrieves a specific sample (image, mask, tabular features, slope, and patient ID) at the given index `idx`.\n",
    "        \"\"\"\n",
    "    BAD_ID = ['ID00011637202177653955184', 'ID00052637202186188008618']\n",
    "    def __init__(self, keys, a, tab):\n",
    "            \n",
    "        \"\"\"\n",
    "        Initializes the OSICData_train dataset.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        keys : list\n",
    "            A list of patient IDs to include in the dataset.\n",
    "        a : dict\n",
    "            A dictionary containing slopes for each patient.\n",
    "        tab : dict\n",
    "            A dictionary containing tabular features for each patient.\n",
    "        \"\"\"\n",
    "        self.keys = [k for k in keys if k not in self.BAD_ID]\n",
    "        \n",
    "        # Store slopes (a) and tabular features (tab)\n",
    "        self.a = a\n",
    "        self.tab = tab\n",
    "        \n",
    "        # Initialize empty lists and dictionaries for storing data\n",
    "        self.all_data = []\n",
    "        self.train_data = {}\n",
    "        self.mask_data = {}\n",
    "        \n",
    "        # Loop through each patient ID in the filtered list of keys\n",
    "        for p in self.keys:  \n",
    "            # Get the image range (min and max slices) for the current patient from the images_range DataFrame\n",
    "            properties = images_range[images_range['ID'] == p]\n",
    "            min_slice = properties['min']\n",
    "            max_slice = properties['max'] \n",
    "\n",
    "            # List the image files and mask files for the patient, excluding the first and last 15% slices\n",
    "            self.train_data[p] = os.listdir(f'{root_path}/train/{p}/')[int(min_slice.iloc[0]):int(max_slice.iloc[0])] \n",
    "            self.mask_data[p] = os.listdir(f'{root_path}/mask_clear/{p}/')[int(min_slice.iloc[0]):int(max_slice.iloc[0])] \n",
    "            \n",
    "            # Add each image file path to the all_data list\n",
    "            for m in self.train_data[p]:\n",
    "                self.all_data.append(p + '/' + m)\n",
    "            \n",
    "        # Sample 50% of the data randomly for training\n",
    "        length = int(0.5 * len(self.all_data))\n",
    "        self.ten_percent = random.sample(self.all_data, length)\n",
    "\n",
    "    # Define the length of the dataset (number of samples)\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of samples in the dataset.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        int : The number of samples in the dataset (50% of the total available data).\n",
    "        \"\"\"\n",
    "        return len(self.ten_percent)\n",
    "    \n",
    "    # Method to retrieve a specific sample from the dataset\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns the number of samples in the dataset.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        int : The number of samples in the dataset (50% of the total available data).\n",
    "        \"\"\"\n",
    "        # Initialize empty lists to hold masks, images, slopes, and tabular features\n",
    "        masks = []\n",
    "        x = []\n",
    "        a, tab = [], [] \n",
    "        \n",
    "        # Get the patient ID and image file path for the current sample\n",
    "        k = self.ten_percent[idx]\n",
    "        i = k[:25]  # Extract patient ID from the image file path\n",
    "       \n",
    "        try:\n",
    "            # Construct the corresponding mask file path by replacing the extension with .jpg\n",
    "            j = k[:-4] + '.jpg'\n",
    "            \n",
    "            # Load the image and mask using the get_img and get_mask functions\n",
    "            img = get_img(f'{root_path}/train/{k}')\n",
    "            mask = get_mask(f'{root_path}/mask_clear/{j}')\n",
    "\n",
    "            # Append the mask, image, slope, and tabular features to the respective lists\n",
    "            masks.append(mask)\n",
    "            x.append(img)\n",
    "            a.append(self.a[i])\n",
    "            tab.append(self.tab[i])\n",
    "        except:\n",
    "            # Print a message if there is an error loading the image or mask\n",
    "            print(k, j)\n",
    "\n",
    "        # Convert the masks, images, slopes, and tabular features into PyTorch tensors\n",
    "        masks, x, a, tab = torch.tensor(np.asanyarray(masks), dtype=torch.float32), torch.tensor(np.asanyarray(x), dtype=torch.float32), torch.tensor(np.asanyarray(a), dtype=torch.float32), torch.tensor(np.asanyarray(tab), dtype=torch.float32)\n",
    "        \n",
    "        # Remove the extra dimension from the tabular feature tensor\n",
    "        tab = torch.squeeze(tab, axis=0)\n",
    "\n",
    "        # Return the mask, image, and tabular feature tensors, along with the slope and patient ID\n",
    "        return [masks, x, tab], a, k \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OSICData_test(torch.utils.data.Dataset):\n",
    "    # List of patient IDs to exclude from the dataset\n",
    "    BAD_ID = ['ID00011637202177653955184', 'ID00052637202186188008618']\n",
    "    \n",
    "    def __init__(self, keys, a, tab):\n",
    "        # Filter out bad patient IDs from the dataset\n",
    "        self.keys = [k for k in keys if k not in self.BAD_ID]\n",
    "        self.a = a  # Additional metadata for patients\n",
    "        self.tab = tab  # Tabular data associated with patients\n",
    "\n",
    "        self.train_data = {}  # Dictionary to store training image file names per patient\n",
    "        self.mask_data = {}   # Dictionary to store corresponding mask file names per patient\n",
    "        \n",
    "        for p in self.keys:  # Iterate over patient IDs\n",
    "            # Retrieve slice range information for the current patient\n",
    "            properties = images_range[images_range['ID'] == p]\n",
    "            min_slice = properties['min']\n",
    "            max_slice = properties['max'] \n",
    "\n",
    "            # Get the total number of slices available for the patient\n",
    "            p_n = len(os.listdir(f'{root_path}/train/{p}/'))\n",
    "\n",
    "            # Select a subset of slices by removing a percentage from the beginning and end\n",
    "            self.train_data[p] = os.listdir(f'{root_path}/train/{p}/')[int(hyp.strip_ct * p_n):-int(hyp.strip_ct * p_n)]\n",
    "            self.mask_data[p] = os.listdir(f'{root_path}/mask_clear/{p}/')[int(hyp.strip_ct * p_n):-int(hyp.strip_ct * p_n)]\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of patients in the dataset\n",
    "        return len(self.keys)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        masks = []  # List to store mask images\n",
    "        x = []  # List to store input images\n",
    "        a, tab = [], []  # Lists to store corresponding metadata and tabular data\n",
    "        \n",
    "        k = self.keys[idx]  # Retrieve patient ID based on the provided index\n",
    "\n",
    "        try:\n",
    "            # Randomly select an image from the available slices for the patient\n",
    "            i = np.random.choice(self.train_data[k], size=1)[0]\n",
    "            j = i[:-4] + '.jpg'  # Convert filename to mask format (assuming a different extension)\n",
    "\n",
    "            # Load the image and corresponding mask\n",
    "            img = get_img(f'{root_path}/train/{k}/{i}')\n",
    "            mask = get_mask(f'{root_path}/mask_clear/{k}/{j}')\n",
    "\n",
    "            # Append the data to respective lists\n",
    "            masks.append(mask)\n",
    "            x.append(img)\n",
    "            a.append(self.a[k])\n",
    "            tab.append(self.tab[k])\n",
    "        except:\n",
    "            print(k, i)  # Print the patient ID and image filename in case of an error\n",
    "\n",
    "        # Convert lists to PyTorch tensors\n",
    "        masks = torch.tensor(np.asanyarray(masks), dtype=torch.float32)\n",
    "        x = torch.tensor(np.asanyarray(x), dtype=torch.float32)\n",
    "        a = torch.tensor(np.asanyarray(a), dtype=torch.float32)\n",
    "        tab = torch.tensor(np.asanyarray(tab), dtype=torch.float32)\n",
    "        \n",
    "        # Remove extra dimensions from tabular data\n",
    "        tab = torch.squeeze(tab, axis=0)\n",
    "\n",
    "        return [masks, x, tab], a, k  # Return the data along with the patient ID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identity class\n",
    "class Identity(nn.Module):\n",
    "    # credit: ptrblck\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnlearnableIdentityConvModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UnlearnableIdentityConvModel, self).__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=64,      # Number of input channels\n",
    "            out_channels=64,     # Number of output channels\n",
    "            kernel_size=1,       # 1x1 convolution\n",
    "            stride=1,            # Stride of 1\n",
    "            padding=0,           # No padding\n",
    "            bias=False           # No bias term needed\n",
    "        )\n",
    "        \n",
    "        # Initialize weights to be identity\n",
    "        self.conv.weight.data = torch.eye(64).view(64, 64, 1, 1)\n",
    "        \n",
    "        # Freeze the weights\n",
    "        self.conv.weight.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device\n",
    "gpu = torch.device(f\"cuda:{hyp.gpu_index}\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sparsemax(nn.Module):\n",
    "    \"\"\"\n",
    "    This class is adapted from the TabNet implementation:\n",
    "    https://github.com/dreamquark-ai/tabnet/blob/develop/pytorch_tabnet/tab_network.py\n",
    "\n",
    "    Sparsemax activation function for neural networks.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim=None):\n",
    "        super(Sparsemax, self).__init__()\n",
    "        self.dim = -1 if dim is None else dim\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.transpose(0, self.dim)\n",
    "        original_size = input.size()\n",
    "        input = input.reshape(input.size(0), -1)\n",
    "        input = input.transpose(0, 1)\n",
    "        dim = 1\n",
    "\n",
    "        number_of_logits = input.size(dim)\n",
    "        \n",
    "        input = input - torch.max(input, dim=dim, keepdim=True)[0].expand_as(input)\n",
    "        zs = torch.sort(input=input, dim=dim, descending=True)[0]\n",
    "        range = torch.arange(start=1, end=number_of_logits + 1, device=device,step=1, dtype=input.dtype).view(1, -1)\n",
    "        range = range.expand_as(zs)\n",
    "\n",
    "        bound = 1 + range * zs\n",
    "        cumulative_sum_zs = torch.cumsum(zs, dim)\n",
    "        is_gt = torch.gt(bound, cumulative_sum_zs).type(input.type())\n",
    "        k = torch.max(is_gt * range, dim, keepdim=True)[0]\n",
    "        zs_sparse = is_gt * zs\n",
    "        taus = (torch.sum(zs_sparse, dim, keepdim=True) - 1) / k\n",
    "        taus = taus.expand_as(input)\n",
    "        self.output = torch.max(torch.zeros_like(input), input - taus)\n",
    "        output = self.output\n",
    "        output = output.transpose(0, 1)\n",
    "        output = output.reshape(original_size)\n",
    "        output = output.transpose(0, self.dim)\n",
    "        return output\n",
    "    def backward(self, grad_output):\n",
    "        dim = 1\n",
    "        nonzeros = torch.ne(self.output, 0)\n",
    "        sum = torch.sum(grad_output * nonzeros, dim=dim) / torch.sum(nonzeros, dim=dim)\n",
    "        self.grad_input = nonzeros * (grad_output - sum.expand_as(grad_output))\n",
    "        return self.grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_non_glu(module,inp_dim,out_dim):\n",
    "     \"\"\"\n",
    "    Initialize the weights of a given module using Xavier normal initialization.\n",
    "\n",
    "    Reference:\n",
    "    Adapted from TabNet implementation:\n",
    "    https://github.com/dreamquark-ai/tabnet/blob/develop/pytorch_tabnet/tab_network.py\n",
    "\n",
    "    Args:\n",
    "        module (torch.nn.Module): The module whose weights need initialization.\n",
    "        inp_dim (int): Input dimension of the layer.\n",
    "        out_dim (int): Output dimension of the layer.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "     gain = np.sqrt((inp_dim+out_dim)/np.sqrt(4*inp_dim))\n",
    "     torch.nn.init.xavier_normal_(module.weight, gain=gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GBN(nn.Module):\n",
    "    \"\"\"\n",
    "    Ghost Batch Normalization (GBN).\n",
    "    Adapted from TabNet implementation:\n",
    "    https://github.com/dreamquark-ai/tabnet/blob/develop/pytorch_tabnet/tab_network.py\n",
    "    \"\"\"\n",
    "    def __init__(self, inp, vbs=128, momentum=0.01):\n",
    "        super().__init__()\n",
    "        self.bn = nn.BatchNorm1d(inp, momentum=momentum)\n",
    "        self.vbs = vbs\n",
    "    \n",
    "    def forward(self, x):\n",
    "        chunk = torch.chunk(x, max(1, x.size(0) // self.vbs), 0)\n",
    "        res = [self.bn(y) for y in chunk]\n",
    "        return torch.cat(res, 0)\n",
    "\n",
    "class GLU(nn.Module):\n",
    "    \"\"\" \n",
    "    Gated Linear Unit (GLU) activation \n",
    "    Adapted from TabNet implementation:\n",
    "    https://github.com/dreamquark-ai/tabnet/blob/develop/pytorch_tabnet/tab_network.py\n",
    "    \"\"\"\n",
    "    def __init__(self, inp_dim, out_dim, fc=None, vbs=128):\n",
    "        super().__init__()\n",
    "        self.fc = fc if fc else nn.Linear(inp_dim, out_dim * 2)\n",
    "        self.bn = GBN(out_dim * 2, vbs=vbs)\n",
    "        self.od = out_dim\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.bn(self.fc(x))\n",
    "        return x[:, :self.od] * torch.sigmoid(x[:, self.od:])\n",
    "\n",
    "class FeatureTransformer(nn.Module):\n",
    "    \"\"\" \n",
    "    Feature transformation block using GLU layers \n",
    "    Adapted from TabNet implementation:\n",
    "    https://github.com/dreamquark-ai/tabnet/blob/develop/pytorch_tabnet/tab_network.py\n",
    "    \"\"\"\n",
    "    def __init__(self, inp_dim, out_dim, shared, n_ind, vbs=128):\n",
    "        super().__init__()\n",
    "        self.shared = nn.ModuleList([GLU(inp_dim, out_dim, shared[0], vbs=vbs)]) if shared else None\n",
    "        self.independ = nn.ModuleList([GLU(out_dim, out_dim, vbs=vbs) for _ in range(n_ind)])\n",
    "        self.scale = torch.sqrt(torch.tensor([.5], device=device))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.shared:\n",
    "            x = sum(glu(x) for glu in self.shared) * self.scale\n",
    "        for glu in self.independ:\n",
    "            x = (x + glu(x)) * self.scale\n",
    "        return x\n",
    "\n",
    "class AttentionTransformer(nn.Module):\n",
    "    \"\"\" \n",
    "    Attention-based feature selection \n",
    "    Adapted from TabNet implementation:\n",
    "    https://github.com/dreamquark-ai/tabnet/blob/develop/pytorch_tabnet/tab_network.py\n",
    "    \"\"\"\n",
    "    def __init__(self, inp_dim, out_dim, relax, vbs=128):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(inp_dim, out_dim)\n",
    "        self.bn = GBN(out_dim, vbs=vbs)\n",
    "        self.r = torch.tensor([relax], device=device)\n",
    "    \n",
    "    def forward(self, a, priors):\n",
    "        a = self.bn(self.fc(a))\n",
    "        mask = torch.sigmoid(a * priors)\n",
    "        priors = priors * (self.r - mask)\n",
    "        return mask\n",
    "\n",
    "class DecisionStep(nn.Module):\n",
    "    \"\"\" \n",
    "    Decision step combining feature transformation and attention \n",
    "    Adapted from TabNet implementation:\n",
    "    https://github.com/dreamquark-ai/tabnet/blob/develop/pytorch_tabnet/tab_network.py\n",
    "    \"\"\"\n",
    "    def __init__(self, inp_dim, n_d, n_a, shared, n_ind, relax, vbs=128):\n",
    "        super().__init__()\n",
    "        self.fea_tran = FeatureTransformer(inp_dim, n_d + n_a, shared, n_ind, vbs)\n",
    "        self.atten_tran = AttentionTransformer(n_a, inp_dim, relax, vbs)\n",
    "    \n",
    "    def forward(self, x, a, priors):\n",
    "        mask = self.atten_tran(a, priors)\n",
    "        loss = (-mask * torch.log(mask + 1e-10)).mean()\n",
    "        x = self.fea_tran(x * mask)\n",
    "        return x, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabCT(nn.Module):\n",
    "    def __init__(self, cnn,num_features=4, feature_dim=40, output_dim=20, num_decision_steps=3,  # 2 boud\n",
    "                    relaxation_factor=1, batch_momentum=0.1, epsilon=0.00001, vgg_npy_path = None\n",
    "                    , n_shared=2, n_d=64, n_a=64, n_ind=2, n_steps=4,relax=1.2,vbs=128):\n",
    "        super(TabCT, self).__init__()\n",
    "        \n",
    "        if n_shared>0:\n",
    "            self.shared = nn.ModuleList()\n",
    "            self.shared.append(nn.Linear(num_features,2*(n_d+n_a)))\n",
    "            for x in range(n_shared-1):\n",
    "                self.shared.append(nn.Linear(n_d+n_a,2*(n_d+n_a)))\n",
    "        else:\n",
    "            self.shared=None\n",
    "        self.first_step = FeatureTransformer(num_features,n_d+n_a,self.shared,n_ind) \n",
    "        self.steps = nn.ModuleList()\n",
    "        for x in range(n_steps-1):\n",
    "            self.steps.append(DecisionStep(num_features,n_d,n_a,self.shared,n_ind,relax,vbs))\n",
    "        self.fc_tab = nn.Linear(n_d,output_dim)\n",
    "        self.bn = nn.BatchNorm1d(num_features)\n",
    "        self.n_d = n_d\n",
    "\n",
    "        # CT features\n",
    "        cnn_dict = {'vit_base_patch16lung0' :None, 'vit_b_16':None, 'vgg16':models.vgg16, 'resnet18': models.resnet18, 'resnet34': models.resnet34, 'resnet50': models.resnet50,\n",
    "                   'resnet101': models.resnet101, 'resnet152': models.resnet152, 'resnext50': models.resnext50_32x4d,\n",
    "                   'resnext101': models.resnext101_32x8d}\n",
    "        '''\n",
    "        fully conected \n",
    "        self.fullyconected = nn.Linear(5, 25)\n",
    "        '''\n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "\n",
    "\n",
    "        self.num_features = num_features\n",
    "        self.feature_dim = feature_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_decision_steps = num_decision_steps\n",
    "        self.relaxation_factor = relaxation_factor\n",
    "        self.batch_momentum = batch_momentum\n",
    "        # self.virtual_batch_size = virtual_batch_size\n",
    "        # self.num_classes = num_classes\n",
    "        self.epsilon = epsilon\n",
    "        # feature dim\n",
    "        self.out_dict = {'vit_base_patch16lung0':768, 'vit_b_16': 768, 'resnet18': 512, 'resnet34': 512, 'resnet50': 2048, 'resnet101': 2048, 'resnet152': 2048,\n",
    "                         'resnext50': 2048, 'resnext101': 2048, \"efnb0\": 1280, \"efnb1\": 1280, \"efnb2\": 1408, \n",
    "                          \"efnb3\": 1536, \"efnb4\": 1792, \"efnb5\": 2048, \"efnb6\": 2304, \"efnb7\": 2560, \"vgg16\": 512}\n",
    "        \n",
    "        self.n_tab = hyp.n_tab # n tabular features\n",
    "        \n",
    "        # efficient net b0 to b7\n",
    "        \n",
    "        if cnn in cnn_dict.keys(): # resnet or resnext or vgg16\n",
    "            if cnn == 'vgg16':\n",
    "                # load vgg16 model badan inja be khatere none noudan ye if bezar\n",
    "\n",
    "                self.ct_cnn = cnn_dict[cnn](pretrained = True).features[2:]\n",
    "                self.conv = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "                self.W = nn.Parameter(torch.nn.init.trunc_normal_(torch.empty((64, 3, 3, 3)), mean=0, std=0.01))\n",
    "                self.B = nn.Parameter(torch.nn.init.trunc_normal_(torch.empty(64), mean=0, std=0.01))\n",
    "\n",
    "            elif cnn == 'vit_b_16':\n",
    "\n",
    "\n",
    "                # model\n",
    "                self.con1 = ViTHybridModel.from_pretrained(\"google/vit-hybrid-base-bit-384\")\n",
    "                # print(\"model\")          \n",
    "\n",
    "\n",
    "                # change configuration \n",
    "                self.input_channel_after_mul = 64\n",
    "                self.input_size_after_mul = 192\n",
    "\n",
    "                self.con1.config.num_channels = self.input_channel_after_mul\n",
    "                self.con1.config.image_size = self.input_size_after_mul \n",
    "\n",
    "                self.con1.embeddings.patch_embeddings.num_channels = self.input_channel_after_mul\n",
    "                self.con1.embeddings.patch_embeddings.image_size = (self.input_size_after_mul , self.input_size_after_mul)\n",
    "\n",
    "                self.con1.embeddings.patch_embeddings.backbone.bit.config.num_channels = self.input_channel_after_mul\n",
    "\n",
    "                self.con1.embeddings.patch_embeddings.backbone.bit.embedder.num_channels = self.input_channel_after_mul\n",
    "\n",
    "                # First CNN Layer\n",
    "                self.conv = self.con1.embeddings.patch_embeddings.backbone.bit.embedder.convolution\n",
    "                # print(\"First CNN Layer\")\n",
    "\n",
    "                # First Mask Layer\n",
    "                self.W = nn.Parameter(torch.nn.init.trunc_normal_(torch.empty((64, 3, 7, 7)), mean=0, std=0.01))\n",
    "                self.B = nn.Parameter(torch.nn.init.trunc_normal_(torch.empty(64), mean=0, std=0.01))\n",
    "                self.mask = self.con1.embeddings.patch_embeddings.backbone.bit.embedder.convolution\n",
    "                self.mask.weight = self.W\n",
    "                self.mask.bias = self.B\n",
    "                # print(\"First Mask Layer\")\n",
    "\n",
    "                # Rest of Embeddings\n",
    "                self.embeddings = self.con1.embeddings\n",
    "                self.embeddings.patch_embeddings.backbone.bit.embedder.convolution = UnlearnableIdentityConvModel()\n",
    "                # print(self.embeddings)\n",
    "                # print(\"Rest of Embeddings\")\n",
    "\n",
    "                # Encoder(ViT), layernorm, pooler layer\n",
    "                self.ct_cnn = self.con1.encoder\n",
    "                # print(self.ct_cnn)\n",
    "                self.norm = self.con1.layernorm\n",
    "                self.pooler = self.con1.pooler\n",
    "                # print(\"Encoder(ViT), layernorm, pooler layer\")\n",
    "\n",
    "\n",
    "\n",
    "            elif cnn == 'vit_base_patch16lung0':\n",
    "                self.con1 = AutoModelForImageClassification.from_pretrained(\"DunnBC22/vit-base-patch16-224-in21k_lung_and_colon_cancer\").vit\n",
    "                self.conv_layer = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3, stride=3, padding=80)\n",
    "                self.conv = self.con1.embeddings.patch_embeddings.backbone.embedder# nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))# self.ct_cnn.conv_proj \n",
    "                self.ct_cnn = self.con1.encoder\n",
    "\n",
    "                self.W = nn.Parameter(torch.nn.init.trunc_normal_(torch.empty((768, 3, 3, 3)), mean=0, std=0.01))\n",
    "                self.B = nn.Parameter(torch.nn.init.trunc_normal_(torch.empty(768), mean=0, std=0.01))\n",
    "                self.mask = self.con1.embeddings\n",
    "                self.mask.patch_embeddings.projection.weight = self.W\n",
    "                self.mask.patch_embeddings.projection.bias = self.B\n",
    "\n",
    "                self.norm = self.con1.layernorm\n",
    "                self.pooler = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\").pooler\n",
    "            else:   \n",
    "                self.ct_cnn = cnn_dict[cnn](pretrained = True)\n",
    "                \n",
    "                # make single channel\n",
    "                self.conv = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "                self.ct_cnn.conv1 = nn.Identity()\n",
    "                self.W = nn.Parameter(torch.nn.init.trunc_normal_(torch.empty((64, 3, 3, 3)), mean=0, std=0.01))\n",
    "                self.B = nn.Parameter(torch.nn.init.trunc_normal_(torch.empty(64), mean=0, std=0.01))\n",
    "                # self.ct_cnn.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "                \n",
    "                # remove the fc layer/ add a simple linear layer\n",
    "                self.ct_cnn.fc = nn.Linear(self.out_dict[cnn], hyp.cnn_dim)   # mapped to 64 dimensions, Identity()\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"cnn not recognized\")\n",
    "        \n",
    "        # second feature extractor\n",
    "        self.ct_cnn_s = models.resnet18(pretrained = True)\n",
    "        self.conv_s = nn.Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.ct_cnn_s.conv1 = nn.Identity()\n",
    "        self.W_s = nn.Parameter(torch.nn.init.trunc_normal_(torch.empty((64, 3, 7, 7)), mean=0, std=0.01))\n",
    "        self.B_s = nn.Parameter(torch.nn.init.trunc_normal_(torch.empty(64), mean=0, std=0.01))\n",
    "        self.mask_s = nn.Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.mask_s.weight = self.W_s\n",
    "        self.mask_s.bias = self.B_s\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        print(\"second feature extractor\")\n",
    "        \n",
    "        self.fc_inter = nn.Linear(hyp.cnn_dim_s + self.n_tab + hyp.cnn_dim, hyp.fc_dim) \n",
    "\n",
    "        self.BN_fc_inter = nn.BatchNorm1d(hyp.fc_dim,\n",
    "                                        momentum=self.batch_momentum)\n",
    "\n",
    "        self.fc = nn.Linear(hyp.fc_dim, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x_ct, x_tab, masks):\n",
    "\n",
    "        x_temp = self.bn(x_tab)\n",
    "        x_a = self.first_step(x_temp)[:,self.n_d:]\n",
    "        loss = torch.zeros(1).to(x_temp.device)\n",
    "        out = torch.zeros(x_temp.size(0),self.n_d).to(x_temp.device)\n",
    "        priors = torch.ones(x_temp.shape).to(x_temp.device)\n",
    "        for step in self.steps:\n",
    "            x_te,l = step(x_temp,x_a,priors)\n",
    "            out += nn.functional.relu(x_te[:,:self.n_d])\n",
    "            x_a = x_te[:,self.n_d:]\n",
    "            loss += l\n",
    "        # all_loss = []\n",
    "        # self.all_loss.append(loss)\n",
    "        # print(\"tabular finished\")\n",
    "        # 1 + 1 + 1\n",
    "        x_ct = torch.cat((x_ct, torch.cat((x_ct, x_ct), 1)), 1)\n",
    "        masks = torch.cat((masks, torch.cat((masks, masks), 1)), 1)\n",
    "        # print(\"input concatenate\")\n",
    "\n",
    "\n",
    "        feature_map = self.conv(x_ct) # ViT\n",
    "        feature_map_s = self.conv_s(x_ct) # CNN\n",
    "        # print(\"first layer image\")\n",
    "\n",
    "        \n",
    "        relevance_map_s = self.mask_s(masks) # CNN\n",
    "        relevance_map = self.mask(masks)  #self.B # ViT\n",
    "        # print(\"first layer mask\")\n",
    "\n",
    "        # multiple element-wise\n",
    "        ct_att = torch.mul(feature_map, relevance_map) # ViT\n",
    "        ct_att_s = torch.mul(feature_map_s, relevance_map_s) # CNN\n",
    "        # print(\"multiply both\")\n",
    "\n",
    "        ct_f_s = self.ct_cnn_s(ct_att_s) # CNN \n",
    "        # print(\"rest of CNN\")\n",
    "\n",
    "        # ViT\n",
    "        # print(ct_att.size())\n",
    "        # print(ct_att.shape)\n",
    "        ct_f = self.embeddings(ct_att)\n",
    "        # print(\"embeddings\")\n",
    "        ct_f = self.ct_cnn(ct_f)\n",
    "        # print(\"ct_cnn\")\n",
    "        # print(ct_f)\n",
    "        # print(ct_f['last_hidden_state'].size())\n",
    "        # print(type(ct_f))\n",
    "        ct_f = self.norm(ct_f['last_hidden_state']) # ct features\n",
    "        # print(\"norm\")\n",
    "        ct_f = self.pooler(ct_f)\n",
    "        # print(\"pooler\")\n",
    "        # print(\"rest of ViT\")\n",
    "\n",
    "        # concatenate\n",
    "        x = torch.cat((ct_f_s, self.fc_tab(out)), -1) # concat on last axis output_aggregated  # changed\n",
    "        x = torch.cat((ct_f, x), -1) # concat on last axis #changed\n",
    "        # print(\"concatenate outputs\")\n",
    "\n",
    "        # dropout\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc_inter(x)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabCT(nn.Module):\n",
    "    def __init__(self, cnn, num_features=4, output_dim=20, \n",
    "                 n_shared=2, n_d=64, n_a=64, n_ind=2, n_steps=4, relax=1.2, vbs=128):\n",
    "        super(TabCT, self).__init__()\n",
    "\n",
    "        # If shared layers are defined, create them\n",
    "        if n_shared > 0:\n",
    "            self.shared = nn.ModuleList()\n",
    "            # First shared layer from num_features to 2*(n_d+n_a)\n",
    "            self.shared.append(nn.Linear(num_features, 2*(n_d + n_a)))\n",
    "            for _ in range(n_shared - 1):\n",
    "                # Additional shared layers with dimensions (n_d+n_a)\n",
    "                self.shared.append(nn.Linear(n_d + n_a, 2*(n_d + n_a)))\n",
    "        else:\n",
    "            self.shared = None\n",
    "        \n",
    "        # First feature transformer step\n",
    "        self.first_step = FeatureTransformer(num_features, n_d + n_a, self.shared, n_ind)\n",
    "        \n",
    "        # Additional decision steps\n",
    "        self.steps = nn.ModuleList()\n",
    "        for _ in range(n_steps - 1):\n",
    "            # Decision steps apply transformation to feature representation\n",
    "            self.steps.append(DecisionStep(num_features, n_d, n_a, self.shared, n_ind, relax, vbs))\n",
    "        \n",
    "        # Fully connected layer for tabular features to output_dim\n",
    "        self.fc_tab = nn.Linear(n_d, output_dim)\n",
    "        \n",
    "        # Batch normalization for tabular features\n",
    "        self.bn = nn.BatchNorm1d(num_features)\n",
    "        \n",
    "        # Save the dimensions of features (n_d)\n",
    "        self.n_d = n_d\n",
    "\n",
    "        # # CNN Model dictionary with different backbone models (e.g., resnet, vgg, etc.)\n",
    "        # cnn_dict = {'vit_base_patch16lung0': None, 'vit_b_16': None, 'vgg16': models.vgg16, \n",
    "        #             'resnet18': models.resnet18, 'resnet34': models.resnet34, \n",
    "        #             'resnet50': models.resnet50, 'resnet101': models.resnet101, \n",
    "        #             'resnet152': models.resnet152, 'resnext50': models.resnext50_32x4d, \n",
    "        #             'resnext101': models.resnext101_32x8d}\n",
    "        \n",
    "        # Dictionary for output dimensions of different models\n",
    "        self.out_dict = {'vit_base_patch16lung0': 768, 'vit_b_16': 768, 'resnet18': 512, 'resnet34': 512, \n",
    "                         'resnet50': 2048, 'resnet101': 2048, 'resnet152': 2048, 'resnext50': 2048, \n",
    "                         'resnext101': 2048, \"efnb0\": 1280, \"efnb1\": 1280, \"efnb2\": 1408, \"efnb3\": 1536, \n",
    "                         \"efnb4\": 1792, \"efnb5\": 2048, \"efnb6\": 2304, \"efnb7\": 2560, \"vgg16\": 512}\n",
    "\n",
    "        # Number of tabular features\n",
    "        self.n_tab = hyp.n_tab  # n tabular features\n",
    "\n",
    "        # Initialize the appropriate CNN model based on input 'cnn'\n",
    "        # if cnn in cnn_dict.keys():\n",
    "            # if cnn == 'vgg16':\n",
    "            #     # For VGG16, customize CNN architecture by extracting layers\n",
    "            #     self.ct_cnn = cnn_dict[cnn](pretrained=True).features[2:]\n",
    "            #     self.conv = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "            #     self.W = nn.Parameter(torch.nn.init.trunc_normal_(torch.empty((64, 3, 3, 3)), mean=0, std=0.01))\n",
    "            #     self.B = nn.Parameter(torch.nn.init.trunc_normal_(torch.empty(64), mean=0, std=0.01))\n",
    "            # elif cnn == 'vit_b_16':\n",
    "                # For Vision Transformer (ViT), use pre-trained model and modify configuration\n",
    "        self.con1 = ViTHybridModel.from_pretrained(\"google/vit-hybrid-base-bit-384\")\n",
    "        self.input_channel_after_mul = 64\n",
    "        self.input_size_after_mul = 192\n",
    "        # Adjust ViT configuration to work with our input size and channels\n",
    "        self.con1.config.num_channels = self.input_channel_after_mul\n",
    "        self.con1.config.image_size = self.input_size_after_mul\n",
    "        self.ct_cnn = self.con1.encoder\n",
    "        self.norm = self.con1.layernorm\n",
    "        self.pooler = self.con1.pooler\n",
    "            # elif cnn == 'vit_base_patch16lung0':\n",
    "            #     # Load a pre-trained ViT model for lung and colon cancer classification\n",
    "            #     self.con1 = AutoModelForImageClassification.from_pretrained(\"DunnBC22/vit-base-patch16-224-in21k_lung_and_colon_cancer\").vit\n",
    "            #     self.ct_cnn = self.con1.encoder\n",
    "            #     self.norm = self.con1.layernorm\n",
    "            #     self.pooler = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\").pooler\n",
    "            # else:\n",
    "            #     # For other models, initialize with pre-trained weights\n",
    "            #     self.ct_cnn = cnn_dict[cnn](pretrained=True)\n",
    "            #     self.conv = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "            #     self.ct_cnn.conv1 = nn.Identity()\n",
    "            #     self.W = nn.Parameter(torch.nn.init.trunc_normal_(torch.empty((64, 3, 3, 3)), mean=0, std=0.01))\n",
    "            #     self.B = nn.Parameter(torch.nn.init.trunc_normal_(torch.empty(64), mean=0, std=0.01))\n",
    "            #     self.ct_cnn.fc = nn.Linear(self.out_dict[cnn], hyp.cnn_dim)\n",
    "\n",
    "        # Secondary feature extractor (ResNet18)\n",
    "        self.ct_cnn_s = models.resnet18(pretrained=True)\n",
    "        self.conv_s = nn.Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.ct_cnn_s.conv1 = nn.Identity()\n",
    "        self.W_s = nn.Parameter(torch.nn.init.trunc_normal_(torch.empty((64, 3, 7, 7)), mean=0, std=0.01))\n",
    "        self.B_s = nn.Parameter(torch.nn.init.trunc_normal_(torch.empty(64), mean=0, std=0.01))\n",
    "        self.mask_s = nn.Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.mask_s.weight = self.W_s\n",
    "        self.mask_s.bias = self.B_s\n",
    "\n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "        # Intermediate fully connected layer\n",
    "        self.fc_inter = nn.Linear(hyp.cnn_dim_s + self.n_tab + hyp.cnn_dim, hyp.fc_dim)\n",
    "    \n",
    "\n",
    "        # Final fully connected layer to predict output\n",
    "        self.fc = nn.Linear(hyp.fc_dim, 1)\n",
    "\n",
    "    def forward(self, x_ct, x_tab, masks):\n",
    "        \"\"\"\n",
    "        Forward pass of the TabCT model.\n",
    "        x_ct: Input CT scan images\n",
    "        x_tab: Input tabular features (e.g., clinical data)\n",
    "        masks: Masks for attention mechanisms in the model\n",
    "        \"\"\"\n",
    "        # Batch normalization for tabular features\n",
    "        x_temp = self.bn(x_tab)\n",
    "        \n",
    "        # First feature transformation step\n",
    "        x_a = self.first_step(x_temp)[:, self.n_d:]\n",
    "        \n",
    "        # Initialize loss and output tensors\n",
    "        loss = torch.zeros(1).to(x_temp.device)\n",
    "        out = torch.zeros(x_temp.size(0), self.n_d).to(x_temp.device)\n",
    "        priors = torch.ones(x_temp.shape).to(x_temp.device)\n",
    "        \n",
    "        # Process through additional decision steps\n",
    "        for step in self.steps:\n",
    "            x_te, l = step(x_temp, x_a, priors)\n",
    "            out += nn.functional.relu(x_te[:, :self.n_d])\n",
    "            x_a = x_te[:, self.n_d:]\n",
    "            loss += l\n",
    "        \n",
    "        # Concatenate CT scan features and duplicate masks for processing\n",
    "        x_ct = torch.cat((x_ct, torch.cat((x_ct, x_ct), 1)), 1)\n",
    "        masks = torch.cat((masks, torch.cat((masks, masks), 1)), 1)\n",
    "        \n",
    "        # Pass the concatenated input through the first CNN layer\n",
    "        feature_map = self.conv(x_ct)  # ViT model\n",
    "        feature_map_s = self.conv_s(x_ct)  # CNN model\n",
    "\n",
    "        # Apply attention mechanism using masks\n",
    "        relevance_map_s = self.mask_s(masks)  # CNN mask\n",
    "        relevance_map = self.mask(masks)  # ViT mask\n",
    "\n",
    "        # Apply element-wise multiplication (attention) to the feature maps\n",
    "        ct_att = torch.mul(feature_map, relevance_map)  # ViT\n",
    "        ct_att_s = torch.mul(feature_map_s, relevance_map_s)  # CNN\n",
    "        \n",
    "        # Process CNN feature map\n",
    "        ct_f_s = self.ct_cnn_s(ct_att_s)\n",
    "\n",
    "        # Process ViT feature map through embeddings, encoder, normalization, and pooler\n",
    "        ct_f = self.embeddings(ct_att)\n",
    "        ct_f = self.ct_cnn(ct_f)\n",
    "        ct_f = self.norm(ct_f['last_hidden_state'])  # ViT features\n",
    "        ct_f = self.pooler(ct_f)\n",
    "\n",
    "        # Concatenate both CNN and ViT outputs\n",
    "        x = torch.cat((ct_f_s, self.fc_tab(out)), -1)  # Concatenated feature map\n",
    "        x = torch.cat((ct_f, x), -1)  # Concatenate final feature map\n",
    "\n",
    "        # Apply dropout for regularization\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Final fully connected layers\n",
    "        x = self.fc_inter(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "def score(fvc_true, fvc_pred, sigma):\n",
    "    sigma_clip = np.maximum(sigma, 70)\n",
    "    delta = np.abs(fvc_true - fvc_pred)\n",
    "    delta = np.minimum(delta, 1000)\n",
    "    sq2 = np.sqrt(2)\n",
    "    metric = (delta / sigma_clip)*sq2 + np.log(sigma_clip* sq2)\n",
    "    return np.mean(metric)\n",
    "\n",
    "\n",
    "def score_avg(p, a): # patient id, predicted a\n",
    "    percent_true = train.Percent.values[train.Patient == p]\n",
    "    fvc_true = train.FVC.values[train.Patient == p]\n",
    "    weeks_true = train.Weeks.values[train.Patient == p]\n",
    "\n",
    "    fvc = a * (weeks_true - weeks_true[0]) + fvc_true[0]\n",
    "    percent = percent_true[0] - a * abs(weeks_true - weeks_true[0])\n",
    "    return score(fvc_true, fvc, percent)\n",
    "\n",
    "def rmse_avg(p, a): # patient id, predicted a\n",
    "    percent_true = train.Percent.values[train.Patient == p]\n",
    "    fvc_true = train.FVC.values[train.Patient == p]\n",
    "    weeks_true = train.Weeks.values[train.Patient == p]\n",
    "\n",
    "    fvc = a * (weeks_true - weeks_true[0]) + fvc_true[0]\n",
    "    return root_mean_squared_error(fvc_true, fvc)\n",
    "\n",
    "\n",
    "\n",
    "def smape(targets, outs):\n",
    "    denominator = (np.abs(targets) + np.abs(outs)) / 2\n",
    "    ape = np.abs(targets - outs) / denominator\n",
    "    return np.mean(ape) * 100\n",
    "\n",
    "# def test_r2_score(p, a):\n",
    "#     fvc_true = train.FVC.values[train.Patient == p]\n",
    "# test_r2_score = r2_score(p.numpy(), pred_a[p].numpy())\n",
    "# test_mape_score = torch.mean(torch.abs((p - pred_a[p]) / p)) * 100\n",
    "# test_mae_score = torch.mean(torch.abs(pred_a[p] - p))\n",
    "# test_smape_score = smape(p, pred_a[p])\n",
    "\n",
    "def plot_figures(figures, nrows = 1, ncols=1):\n",
    "    \"\"\"Plot a dictionary of figures.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    figures : <title, figure> dictionary\n",
    "    ncols : number of columns of subplots wanted in the display\n",
    "    nrows : number of rows of subplots wanted in the figure\n",
    "    \"\"\"\n",
    "\n",
    "    fig, axeslist = plt.subplots(ncols=ncols, nrows=nrows)\n",
    "    for ind,title in enumerate(figures):\n",
    "        axeslist.ravel()[ind].imshow(figures[title], cmap=plt.gray())\n",
    "        axeslist.ravel()[ind].set_title(title)\n",
    "        axeslist.ravel()[ind].set_axis_off()\n",
    "    plt.tight_layout() # optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "result_dir = hyp.results_dir\n",
    "\n",
    "# training only resnet models on gpu 0\n",
    "train_models = hyp.train_models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_s, test = train_test_split(P, test_size=0.2, random_state=57)\n",
    "len(train_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nfold = hyp.nfold # hyper\n",
    "\n",
    "# removing noisy data\n",
    "P = [p for p in P if p not in ['ID00011637202177653955184', 'ID00052637202186188008618']] # in bayad bere bala\n",
    "\n",
    "for model in train_models:\n",
    "    log = open(f\"{result_dir}/september8th/{model}simplestep2hybrid.txt\", \"a+\")\n",
    "    kfold =KFold(n_splits=nfold)\n",
    "    \n",
    "    ifold = 0\n",
    "    min_sq = {}\n",
    "\n",
    "    for train_index, valid_index in kfold.split(train_s):  \n",
    "        \n",
    "        p_train = np.array(P)[train_index] \n",
    "        p_valid = np.array(P)[valid_index] \n",
    "        print(len(p_train))\n",
    "        osic_train = OSICData_train(p_train, A, TAB)\n",
    "        train_loader = torch.utils.data.DataLoader(osic_train, batch_size=hyp.batch_size, shuffle=True, num_workers=hyp.num_workers, drop_last=True)\n",
    "\n",
    "        osic_val = OSICData_test(p_valid, A, TAB)\n",
    "        val_loader = torch.utils.data.DataLoader(osic_val, batch_size=hyp.batch_size, shuffle=True, num_workers=hyp.num_workers)\n",
    "        print(len(osic_train))\n",
    "        print(len(train_loader))\n",
    "        print(len(val_loader))\n",
    "\n",
    "        tabct = TabCT(cnn = model).to(gpu)\n",
    "        print(f\"creating {model}\")\n",
    "        print(f\"fold: {ifold}\")\n",
    "        log.write(f\"fold: {ifold}\\n\")\n",
    "\n",
    "        \n",
    "\n",
    "        n_epochs = hyp.n_epochs # max 30 epochs, patience 5, find the suitable epoch number for later final training\n",
    "\n",
    "        best_epoch = n_epochs # 30\n",
    "\n",
    "\n",
    "        optimizer = torch.optim.AdamW(tabct.parameters())\n",
    "        criterion = torch.nn.L1Loss()\n",
    "\n",
    "        max_score = 99999999.0000 # here, max score ]= minimum score\n",
    "        tot_rmse = []\n",
    "        for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
    "            running_loss = 0.0\n",
    "            tabct.train()\n",
    "\n",
    "            tabular_loss = []\n",
    "            for i, data in enumerate(tqdm(train_loader, 0)):\n",
    "\n",
    "                [mask, x, t], a, _ = data\n",
    "\n",
    "                x = x.to(gpu)\n",
    "                mask = mask.to(gpu)\n",
    "                t = t.to(gpu)\n",
    "                a = a.to(gpu)\n",
    "                # print(x)\n",
    "                # print(t)\n",
    "                # print(mask)\n",
    "                # t hanoun tabe\n",
    "                # ultimate = ultimate.to(gpu)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # forward + backward + optimize\n",
    "                outputs, tab_loss = tabct(x, t, mask) # here\n",
    "                # print(outputs.size())\n",
    "                tabular_loss.append(tab_loss)\n",
    "                loss = criterion(outputs, a)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # print statistics\n",
    "                running_loss += loss.item()\n",
    "            print(f\"tabular loss: {tabular_loss}\")\n",
    "            print(f\"epoch {epoch+1} train: {running_loss}\")\n",
    "            log.write(f\"epoch {epoch+1} train: {running_loss}\\n\")\n",
    "\n",
    "\n",
    "            running_loss = 0.0\n",
    "            pred_a = {}\n",
    "            tabct.eval()\n",
    "            tabular_loss = []\n",
    "            for i, data in enumerate(tqdm(val_loader, 0)):\n",
    "\n",
    "                [mask, x, t], a, pid = data\n",
    "\n",
    "                x = x.to(gpu)\n",
    "                mask = mask.to(gpu)\n",
    "                t = t.to(gpu)\n",
    "                a = a.to(gpu)\n",
    "\n",
    "                # forward\n",
    "                outputs, tab_loss = tabct(x, t, mask)\n",
    "                loss = criterion(outputs, a)\n",
    "                tabular_loss.append(tab_loss)\n",
    "                pids = pid\n",
    "                preds_a = outputs.detach().cpu().numpy().flatten()\n",
    "\n",
    "                for j, p_d in enumerate(pids):\n",
    "                    pred_a[p_d] = preds_a[j]\n",
    "\n",
    "               \n",
    "\n",
    "\n",
    "                # print statistics\n",
    "                running_loss += loss.item()\n",
    "            print(tabular_loss)\n",
    "            print(f\"epoch {epoch+1} val: {running_loss}\")\n",
    "            log.write(f\"epoch {epoch+1} val: {running_loss}\\n\")\n",
    "            # score calculation\n",
    "            # print(pred_a)\n",
    "            # print(len(pred_a))\n",
    "            # print(p_test)\n",
    "            # print(len(p_test))\n",
    "\n",
    "            # totals\n",
    "            tot_r2_score = []\n",
    "            tot_mape_score = []\n",
    "            tot_mae_score = []\n",
    "            tot_smape_score = []\n",
    "\n",
    "            # everyone\n",
    "            score_v = 0.\n",
    "            rmse = 0.\n",
    "            test_r2_score = []\n",
    "            test_mape_score = []\n",
    "            test_mae_score = []\n",
    "            test_smape_score = []\n",
    "            print(len(p_valid))\n",
    "            # fvcs_true = []\n",
    "            # fvcs_pred = []\n",
    "            for p in p_valid:\n",
    "                score_v += (score_avg(p, pred_a[p]))/len(p_valid)\n",
    "                rmse += (rmse_avg(p, pred_a[p]))/len(p_valid)\n",
    "                fvc_true = train.FVC.values[train.Patient == p]\n",
    "                weeks_true = train.Weeks.values[train.Patient == p]\n",
    "                fvc_pred = pred_a[p] * (weeks_true - weeks_true[0]) + fvc_true[0]\n",
    "\n",
    "                test_r2_score.append(r2_score(fvc_true, fvc_pred))\n",
    "                test_mape_score.append(np.mean(np.abs((fvc_true - fvc_pred) / fvc_true)) * 100)\n",
    "                test_mae_score.append(np.mean(np.abs(fvc_pred - fvc_true)))\n",
    "                test_smape_score.append(smape(fvc_true, fvc_pred))\n",
    "            #------------------------\n",
    "            tot_rmse.append(rmse)\n",
    "            tot_r2_score.append(np.asanyarray(test_r2_score))\n",
    "            tot_mape_score.append(np.asanyarray(test_mape_score))\n",
    "            tot_mae_score.append(np.asanyarray(test_mae_score))\n",
    "            tot_smape_score.append(np.asanyarray(test_smape_score))\n",
    "            #------------------------\n",
    "\n",
    "            print(\"this is rmse\")\n",
    "            print(tot_rmse)\n",
    "            print(\"this is r2\")\n",
    "            print(tot_r2_score)\n",
    "            print(\"this is mape\")\n",
    "            print(tot_mape_score)\n",
    "            print(\"this is mae\")\n",
    "            print(tot_mae_score)\n",
    "            print(\"this is smape\")\n",
    "            print(tot_smape_score)\n",
    "            print(f\"val score: {score_v}\")\n",
    "            log.write(f\"val score: {score_v}\\n\")\n",
    "            log.write(f\"val rmse: {rmse}\\n\")\n",
    "\n",
    "            if score_v <= max_score:\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': tabct.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'score': score_v\n",
    "                    }, f\"{result_dir}/september8th/{model}simplestep2hybrid.tar\")\n",
    "                max_score = score_v\n",
    "                best_epoch = epoch + 1\n",
    "        min_sq[ifold] = np.array(tot_rmse)\n",
    "        # print(\"all mean square error\")\n",
    "        # print(min_sq)\n",
    "        ifold += 1\n",
    "        # destroy model\n",
    "        del tabct\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# # ref: https://www.kaggle.com/miklgr500/linear-decay-based-on-resnet-cnn\n",
    "# # https://pytorch.org/docs/stable/index.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = [p for p in test if p not in ['ID00011637202177653955184', 'ID00052637202186188008618']]\n",
    "osic_test = OSICData_test(test, A, TAB)\n",
    "test_loader = torch.utils.data.DataLoader(osic_test, batch_size=1, num_workers=hyp.num_workers)\n",
    "\n",
    "\n",
    "# load the best model\n",
    "tabct = TabCT(cnn = model).to(gpu)\n",
    "tabct.load_state_dict(torch.load(f\"{result_dir}/september8th/{model}simplestep2hybrid.tar\")[\"model_state_dict\"])\n",
    "\n",
    "running_loss = 0.0\n",
    "pred_a = {}\n",
    "tabct.eval()\n",
    "tabular_loss = []\n",
    "for i, data in enumerate(tqdm(test_loader, 0)):\n",
    "\n",
    "    [mask, x, t], a, pid = data\n",
    "\n",
    "    x = x.to(gpu)\n",
    "    mask = mask.to(gpu)\n",
    "    t = t.to(gpu)\n",
    "    a = a.to(gpu)\n",
    "\n",
    "    # forward\n",
    "    outputs, tab_loss = tabct(x, t, mask)\n",
    "    loss = criterion(outputs, a)\n",
    "    tabular_loss.append(tab_loss)\n",
    "    pids = pid\n",
    "    preds_a = outputs.detach().cpu().numpy().flatten()\n",
    "    print([outputs, pid])\n",
    "    for j, p_d in enumerate(pids):\n",
    "        pred_a[p_d] = preds_a[j]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # print statistics\n",
    "    running_loss += loss.item()\n",
    "print(tabular_loss)\n",
    "# print(f\"epoch {epoch+1} val: {running_loss}\")\n",
    "# log.write(f\"epoch {epoch+1} val: {running_loss}\\n\")\n",
    "# score calculation\n",
    "# print(pred_a)\n",
    "# print(len(pred_a))\n",
    "# print(p_test)\n",
    "# print(len(p_test))\n",
    "\n",
    "# totals\n",
    "tot_r2_score = []\n",
    "tot_mape_score = []\n",
    "tot_mae_score = []\n",
    "tot_smape_score = []\n",
    "\n",
    "# everyone\n",
    "score_v = 0.\n",
    "rmse = 0.\n",
    "test_r2_score = []\n",
    "test_mape_score = []\n",
    "test_mae_score = []\n",
    "test_smape_score = []\n",
    "# print(len(p_valid))\n",
    "# fvcs_true = []\n",
    "# fvcs_pred = []\n",
    "for p in test:\n",
    "    score_v += (score_avg(p, pred_a[p]))/len(test)\n",
    "    rmse += (rmse_avg(p, pred_a[p]))/len(test)\n",
    "    fvc_true = train.FVC.values[train.Patient == p]\n",
    "    weeks_true = train.Weeks.values[train.Patient == p]\n",
    "    fvc_pred = pred_a[p] * (weeks_true - weeks_true[0]) + fvc_true[0]\n",
    "\n",
    "    test_r2_score.append(r2_score(fvc_true, fvc_pred))\n",
    "    test_mape_score.append(np.mean(np.abs((fvc_true - fvc_pred) / fvc_true)) * 100)\n",
    "    test_mae_score.append(np.mean(np.abs(fvc_pred - fvc_true)))\n",
    "    test_smape_score.append(smape(fvc_true, fvc_pred))\n",
    "#------------------------\n",
    "tot_rmse.append(rmse)\n",
    "tot_r2_score.append(np.asanyarray(test_r2_score))\n",
    "tot_mape_score.append(np.asanyarray(test_mape_score))\n",
    "tot_mae_score.append(np.asanyarray(test_mae_score))\n",
    "tot_smape_score.append(np.asanyarray(test_smape_score))\n",
    "#------------------------\n",
    "\n",
    "print(\"this is rmse\")\n",
    "print(tot_rmse)\n",
    "print(\"this is r2\")\n",
    "print(tot_r2_score)\n",
    "print(\"this is mape\")\n",
    "print(tot_mape_score)\n",
    "print(\"this is mae\")\n",
    "print(tot_mae_score)\n",
    "print(\"this is smape\")\n",
    "print(tot_smape_score)\n",
    "print(f\"val score: {score_v}\")\n",
    "log.write(f\"val score: {score_v}\\n\")\n",
    "log.write(f\"val rmse: {rmse}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ID00025637202179541264076': -4.9629145, 'ID00417637202310901214011': -6.96957, 'ID00423637202312137826377': -5.1295543, 'ID00078637202199415319443': -8.249274, 'ID00042637202184406822975': -4.4956326, 'ID00210637202257228694086': -0.25509533, 'ID00367637202296290303449': -5.776394, 'ID00283637202278714365037': -6.7125416, 'ID00242637202264759739921': -4.6875415, 'ID00329637202285906759848': -4.974105, 'ID00111637202210956877205': -5.1242247, 'ID00135637202224630271439': -6.878868, 'ID00341637202287410878488': 0.0043280534, 'ID00371637202296828615743': -5.0871334, 'ID00048637202185016727717': -4.488312, 'ID00077637202199102000916': -6.0758114, 'ID00305637202281772703145': -4.573623, 'ID00331637202286306023714': -0.99795616, 'ID00110637202210673668310': -5.1420565, 'ID00319637202283897208687': -5.1295567, 'ID00336637202286801879145': -4.531871, 'ID00400637202305055099402': -4.3320036, 'ID00161637202235731948764': -4.636307, 'ID00364637202296074419422': -4.687541, 'ID00344637202287684217717': -6.690444, 'ID00138637202231603868088': -5.576578, 'ID00337637202286839091062': -4.974, 'ID00381637202299644114027': -4.5861263, 'ID00047637202184938901501': -6.0950804, 'ID00383637202300493233675': -4.9870872, 'ID00026637202179561894768': -4.261383, 'ID00168637202237852027833': -4.5861263, 'ID00117637202212360228007': -1.128756, 'ID00407637202308788732304': -5.4980135, 'ID00123637202217151272140': -4.5735016}\n",
      "['ID00025637202179541264076', 'ID00417637202310901214011', 'ID00423637202312137826377', 'ID00078637202199415319443', 'ID00042637202184406822975', 'ID00210637202257228694086', 'ID00367637202296290303449', 'ID00283637202278714365037', 'ID00242637202264759739921', 'ID00329637202285906759848', 'ID00111637202210956877205', 'ID00135637202224630271439', 'ID00341637202287410878488', 'ID00371637202296828615743', 'ID00048637202185016727717', 'ID00077637202199102000916', 'ID00305637202281772703145', 'ID00331637202286306023714', 'ID00110637202210673668310', 'ID00319637202283897208687', 'ID00336637202286801879145', 'ID00400637202305055099402', 'ID00161637202235731948764', 'ID00364637202296074419422', 'ID00344637202287684217717', 'ID00138637202231603868088', 'ID00337637202286839091062', 'ID00381637202299644114027', 'ID00047637202184938901501', 'ID00383637202300493233675', 'ID00026637202179561894768', 'ID00168637202237852027833', 'ID00117637202212360228007', 'ID00407637202308788732304', 'ID00123637202217151272140']\n"
     ]
    }
   ],
   "source": [
    "print(pred_a)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# final training with optimized setting\n",
    "\n",
    "osic_all = OSICData_test(P, A, TAB)\n",
    "all_loader = torch.utils.data.DataLoader(osic_all, batch_size=2, shuffle=True, num_workers=hyp.num_workers)\n",
    "\n",
    "# load the best model\n",
    "tabct = TabCT(cnn = model).to(gpu)\n",
    "tabct.load_state_dict(torch.load(f\"{result_dir}/september8th/{model}simplestep2hybrid.tar\")[\"model_state_dict\"])\n",
    "\n",
    "optimizer = torch.optim.AdamW(tabct.parameters(), lr = hyp.final_lr) # very small learning rate\n",
    "\n",
    "\n",
    "print(f\"Final training\")\n",
    "log.write(f\"Final training\\n\")  \n",
    "for epoch in range(best_epoch + 2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    tabct.train()\n",
    "    for i, data in enumerate(tqdm(all_loader, 0)):\n",
    "\n",
    "        [mask, x, t], a, _ = data\n",
    "\n",
    "        x = x.to(gpu)\n",
    "        mask = mask.to(gpu)\n",
    "        t = t.to(gpu)\n",
    "        a = a.to(gpu)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs, _ = tabct(x, t, mask)\n",
    "        loss = criterion(outputs, a)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "    print(f\"epoch {epoch+1} train: {running_loss}\")\n",
    "    log.write(f\"epoch {epoch+1} train: {running_loss}\\n\")\n",
    "    torch.save({\n",
    "        'epoch': best_epoch,\n",
    "        'model_state_dict': tabct.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "        }, f\"{result_dir}/september8th/{model}simplestep2hybrid.tar\")\n",
    "\n",
    "print('Finished Training')\n",
    "# destroy model\n",
    "del tabct\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# plot_figures(min_sq, 2, 3)\n",
    "\n",
    "# ref: https://www.kaggle.com/miklgr500/linear-decay-based-on-resnet-cnn\n",
    "# https://pytorch.org/docs/stable/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTHybridModel were not initialized from the model checkpoint at google/vit-hybrid-base-bit-384 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "second feature extractor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the best model\n",
    "tabct = TabCT(cnn = model).to(gpu)\n",
    "tabct.load_state_dict(torch.load(f\"{result_dir}/september8th/{model}simplestep2hybrid.tar\")[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "Images saved in directory: slices_by_cmap/VIT/image\n"
     ]
    }
   ],
   "source": [
    "# C:\\Users\\Vison\\Documents\\Users\\Dolatabadi\\FibroCosaNet\\train\\ID00216637202257988213445\\10.dcm\n",
    "image = get_img(\"../train/ID00216637202257988213445/10.dcm\")\n",
    "image = np.stack((image, image, image), axis=-1)\n",
    "image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0) .to(gpu)\n",
    "out = tabct.conv(image).squeeze(0)\n",
    "tensor = out  # Example: Replace with your actual tensor\n",
    "print(type(tensor))\n",
    "# Convert tensor to numpy if needed\n",
    "tensor_np = tensor.to(\"cpu\").detach().numpy()\n",
    "\n",
    "\n",
    "# List of colormaps to visualize\n",
    "colormaps = ['gray', 'bone', 'inferno', 'viridis', 'magma', 'jet', 'hot', 'coolwarm', 'plasma', 'cividis']\n",
    "\n",
    "# Create a folder to save the images\n",
    "save_dir = \"slices_by_cmap/VIT/image\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Plot all slices for each colormap and save the output\n",
    "for cmap in colormaps:\n",
    "    # Create a figure with subplots\n",
    "    fig, axes = plt.subplots(8, 8, figsize=(15, 15))  # 8x8 grid for 64 slices\n",
    "\n",
    "    # Plot each slice in the grid\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            slice_idx = i * 8 + j  # Calculate the current slice index\n",
    "            axes[i, j].imshow(tensor_np[slice_idx], cmap=cmap)\n",
    "            axes[i, j].set_title(f'Channel {slice_idx}')\n",
    "            axes[i, j].axis('off')  # Hide the axis for clarity\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure for the current colormap\n",
    "    save_path = os.path.join(save_dir, f\"channels_{cmap}.png\")\n",
    "    plt.savefig(save_path)\n",
    "\n",
    "    # Close the figure to release memory\n",
    "    plt.close(fig)\n",
    "\n",
    "print(f\"Images saved in directory: {save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "Images saved in directory: slices_by_cmap/VIT/mask\n"
     ]
    }
   ],
   "source": [
    "# C:\\Users\\Vison\\Documents\\Users\\Dolatabadi\\FibroCosaNet\\mask_clear\\10.jpg\n",
    "image = get_mask(\"../mask_clear/ID00216637202257988213445/10.jpg\")\n",
    "image = np.stack((image, image, image), axis=-1)\n",
    "image = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0) .to(gpu)\n",
    "out = tabct.mask(image).squeeze(0)\n",
    "tensor = out  # Example: Replace with your actual tensor\n",
    "print(type(tensor))\n",
    "# Convert tensor to numpy if needed\n",
    "tensor_np = tensor.to(\"cpu\").detach().numpy()\n",
    "\n",
    "\n",
    "# List of colormaps to visualize\n",
    "colormaps = ['gray', 'bone', 'inferno', 'viridis', 'magma', 'jet', 'hot', 'coolwarm', 'plasma', 'cividis']\n",
    "\n",
    "# Create a folder to save the images\n",
    "save_dir = \"slices_by_cmap/VIT/mask\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Plot all slices for each colormap and save the output\n",
    "for cmap in colormaps:\n",
    "    # Create a figure with subplots\n",
    "    fig, axes = plt.subplots(8, 8, figsize=(15, 15))  # 8x8 grid for 64 slices\n",
    "\n",
    "    # Plot each slice in the grid\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            slice_idx = i * 8 + j  # Calculate the current slice index\n",
    "            axes[i, j].imshow(tensor_np[slice_idx], cmap=cmap)\n",
    "            axes[i, j].set_title(f'Channel {slice_idx}')\n",
    "            axes[i, j].axis('off')  # Hide the axis for clarity\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure for the current colormap\n",
    "    save_path = os.path.join(save_dir, f\"Channels_{cmap}.png\")\n",
    "    plt.savefig(save_path)\n",
    "\n",
    "    # Close the figure to release memory\n",
    "    plt.close(fig)\n",
    "\n",
    "print(f\"Images saved in directory: {save_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
